# 7. Embeddings

## What are Embeddings?
- Embeddings are dense vector representations of data, often used for categorical variables or high-dimensional data.
- They allow models to work with large feature spaces efficiently.

## Key Concepts
- **Embedding Space**: A continuous vector space where similar items are close together.
- **Static Embeddings**: Pre-trained and fixed (e.g., word2vec, GloVe).
- **Learned Embeddings**: Learned during model training (e.g., in neural networks).

## Why Use Embeddings?
- Reduce dimensionality of categorical data.
- Capture semantic relationships (e.g., similar words have similar vectors).
- Enable efficient computation and better generalization.

## Example Use Cases
- Natural language processing: Representing words, sentences, or documents.
- Recommender systems: Representing users and items.

## Visual Intuition
- Items with similar meanings or properties are mapped to nearby points in the embedding space.
- Embeddings can be visualized in 2D/3D to show clusters of similar items.

---
Next: 8. Large Language Models 